import os
import sys
import requests
from   bs4 import BeautifulSoup
from   urllib.parse import urljoin, urlparse
import argparse

IMAGE_EXTENSIONS = [".jpg", ".jpeg", ".png", ".gif", ".bmp"]

visited_urls= set()

def download_image(img_url, folder_path, index):
    parsed = urlparse(img_url)
    filename = os.path.basename(parsed.path)
    ext = os.path.splitext(filename)[1].lower()
    if ext not in IMAGE_EXTENSIONS:
        return
    if not filename:
        filename = f"image_{index}{ext}"
    filepath = os.path.join(folder_path, filename)
    try:
        img_data = requests.get(img_url).content
        with open(filepath, 'wb') as f:
            f.write(img_data)
        print(f"[+] Downloaded {filename}")
    except Exception as e:
        print(f"[!] Failed to download {img_url}: {e}")

def crawl(url, folder_path, depth, max_depth):
    if url in visited_urls or depth > max_depth:
        return
    visited_urls.add(url)
    try:
        response = requests.get(url)
        response.raise_for_status()
    except Exaption as e:
        print(f"[!]faild to access {url}: {e}")
        return
    soup = BeautifulSoup(response.text, "html.parser")
    img_tags = soup.find_all("img")
    for i, img in enumerate(img_tags):
        src = img.get("src")
        if src:
            full_img_url = urljoin(url, src)
            download_image(full_img_url, folder_path, i)
    if depth < max_depth:
        for link in soup.find_all("a"):
            href = link.get("href")
            if href and not href.startswith('#'):
                full_link = urljoin(url, href)
                crawl(full_link, folder_path, depth + 1, max_depth)

def main():
    parser = argparse.ArgumentParser(description="Recursive image spider")
    parser.add_argument("url", help="Target URL to start crawling")
    parser.add_argument("-r", action="store_true", help="Enable recursive crawling")
    parser.add_argument("-l", type=int, default=5, help="Max recursion depth (default: 5)")
    parser.add_argument("-p", type=str, default="./data", help="Folder path to save images")
    args = parser.parse_args()
    if not os.path.exists(args.p):
        os.makedirs(args.p)
    if args.r:
        crawl(args.url, args.p, depth=0, max_depth=args.l)
    else:
        crawl(args.url, args.p, depth=0, max_depth=0)

if __name__== "__main__":
    main()
    